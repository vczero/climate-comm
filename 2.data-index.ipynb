{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "401048b2-9f57-4282-a831-1f96ad466510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import datetime\n",
    "from urllib import parse\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_cookie(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            return first_line\n",
    "    except FileNotFoundError:\n",
    "        print(f'文件 {file_path} 不存在。')\n",
    "    except Exception as e:\n",
    "        print(f'读取文件 {file_path} 时出错: {e}')\n",
    "\n",
    "cookie = read_cookie('cookie.txt')\n",
    "\n",
    "class BaiduIndex:\n",
    "    def __init__(self, keyword, start_date, end_date, city=0):\n",
    "        self.city = city\n",
    "        self.keyword = keyword\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "    cookies = {}\n",
    "    headers = {}\n",
    "        \n",
    "    def get_search_data(self):\n",
    "        # For academic research only.\n",
    "        pass\n",
    "\n",
    "            \n",
    "def nationwide(keyword):\n",
    "    start_date = '2015-01-01'\n",
    "    end_date = '2025-10-16' \n",
    "    city_code = 0\n",
    "    \n",
    "    doindex = BaiduIndex(keyword, start_date, end_date, city_code)\n",
    "    search_results = doindex.get_search_data()\n",
    "   \n",
    "    with open('./data/years/search_2015-2025' + keyword + '.json', 'w') as file:\n",
    "        json.dump(search_results, file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    print('====' + keyword + '====')\n",
    "    \n",
    "\n",
    "def do_city_search(city_code, city_name, keyword):\n",
    "    start_date = '2015-01-01'\n",
    "    end_date = '2025-10-16'  \n",
    "    \n",
    "    doindex = BaiduIndex(keyword, start_date, end_date, city_code)\n",
    "    search_results = doindex.get_search_data()\n",
    "    with open('./data/search/' + keyword + '/'+ city_name + '.json', 'w') as file:\n",
    "        json.dump(search_results, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f53554a-215a-4826-ac75-484df6a7db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.read_csv('./keywords.csv')['keyword'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92146b17-a3b6-4d8b-b310-ccf16afbe437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====低碳====\n",
      "====低碳出行====\n",
      "====低碳饮食====\n",
      "====低碳经济====\n",
      "====碳中和====\n",
      "====碳达峰====\n",
      "====碳排放====\n",
      "====碳足迹====\n",
      "====碳交易====\n",
      "====碳汇====\n"
     ]
    }
   ],
   "source": [
    "# 全国尺度数据\n",
    "for key in keywords:\n",
    "    nationwide(key)\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eddd6e14-15fe-47a6-abea-7f5c1a174f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists_in_directory(directory, filename):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    return os.path.isfile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c4f1ab8-e8b4-46cd-9721-d2d3db953cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fetch baidu index: 100%|███████████████████████████████| 367/367 [00:46<00:00,  7.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# 各城市数据\n",
    "bd_citycode = pd.read_csv('./data/baidu_index_citycode_mapping.csv')\n",
    "for i, r in tqdm(bd_citycode.iterrows(), total=len(bd_citycode), desc='fetch baidu index'):\n",
    "    for key in keywords:\n",
    "        is_ok = file_exists_in_directory('./data/search/' + key, r['name'] + '.json')\n",
    "        if is_ok:\n",
    "            pass\n",
    "        else:\n",
    "            do_city_search(r['code'], r['name'], key)\n",
    "            time.sleep(3.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2f00-62f6-46a2-95ff-41bf88e976c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40237fbc-f6d7-4e5d-8ac7-358ac28bc91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
